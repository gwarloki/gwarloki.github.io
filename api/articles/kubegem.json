{"title":"Kubernetes学习笔记 --kubegem","uid":"5d82079790f13b8d3ae4a637793178a7","slug":"kubegem","date":"2022-05-04T16:00:00.000Z","updated":"2022-11-02T02:59:25.298Z","comments":true,"path":"api/articles/kubegem.json","keywords":null,"cover":"https://gtwallpaper.org/sites/default/files/wallpaper/159420/4k-dramatic-sunset-over-blidinje-wallpapers-159420-21754-8191427.png","content":"<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><p><a href=\"https://www.kubegems.io/\"><strong>KubeGems</strong></a>是一款以围绕 Kubernetes 通过自研和集成云原生项目而构建的通用性开源 PaaS 云管理平台。经过我们内部近一年的持续迭代，当前 KubeGems 的核心功能已经初步具备多云多租户场景下的统一管理。并通过插件化的方式，在用户界面中灵活控制包括 <strong>监控系统</strong>、<strong>日志系统</strong>、<strong>微服务治理</strong> 等众多插件的启用和关闭。</p>\n<p>作为一个面向云原生的通用型云平台，<a href=\"https://github.com/kubegems/kubegems\"><strong>KubeGems</strong></a>从立项开始就把支持多集群、多租户场景下的资源隔离作为其主要实现设计目标。用户可以对接入平台的 Kubernetes 集群做 <em>租户级</em> 的自定义资源规划。除此之外，我们提供了比原生 Dashboard 功能更加丰富且人性化操作的 UI 界面，让用户&#x2F;企业根据自身场景规划平台元数据，而不用担心自己的业务和数据出现错乱。同时 KubeGems 也提供过了众多丰富的功能模块来为个人或企业用户带来更好的使用体验，例如 _访问控制、资源规划、网络隔离、租户网关、存储卷、可观察性、用户审计、证书管理、金丝雀发布、istio治理_等功能。</p>\n<h2 id=\"功能预览\"><a href=\"#功能预览\" class=\"headerlink\" title=\"功能预览\"></a>功能预览</h2><p><img src=\"https://s2.loli.net/2022/05/07/XCg37KchvOAy5dx.png\"></p>\n<h2 id=\"quick-start\"><a href=\"#quick-start\" class=\"headerlink\" title=\"quick start\"></a>quick start</h2><p><strong>KubeGems Installer Operator</strong>是一个通过 <a href=\"https://sdk.operatorframework.io/\">Operator SDK</a> 构建的Ansible Kubernetes控制器。通过CRD中的定义的字段对Ansible进行变量传递。Installer Operator的运行需要具备集群管理员的RBAC权限，所以在部署前需要联系您的集群管理员，以保证有足够的授权执行以下操作。<br><img src=\"https://s2.loli.net/2022/05/07/ZeGBhiDu7A4PQ9O.png\"></p>\n<h3 id=\"installer\"><a href=\"#installer\" class=\"headerlink\" title=\"installer\"></a>installer</h3><pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">wget -O - https:&#x2F;&#x2F;github.com&#x2F;kubegems&#x2F;installer-operator&#x2F;releases&#x2F;download&#x2F;$&#123;KUBEGEMS_VERSION&#125;&#x2F;centrol.installer.yaml \\\n| sed &quot;s#repository: docker.io#repository: registry.cn-beijing.aliyuncs.com#g&quot; \\\n| kubectl apply -f -</code></pre>\n<h3 id=\"访问页面\"><a href=\"#访问页面\" class=\"headerlink\" title=\"访问页面\"></a>访问页面</h3><p>创建一个ingress规则将 kubegems dashboard 服务暴露出来，参考如下</p>\n<pre class=\"line-numbers language-yaml\" data-language=\"yaml\"><code class=\"language-yaml\">apiVersion: extensions&#x2F;v1beta1\nkind: Ingress\nmetadata:\n  annotations:\n    nginx.org&#x2F;proxy-buffering: &quot;false&quot;\n    nginx.org&#x2F;websocket-services: gems-dashboard\n  name: gems-dashboard\n  namespace: gemcloud-system\nspec:\n  rules:\n  - host: console.kubegems.io\n    http:\n      paths:\n      - backend:\n          serviceName: gems-dashboard\n          servicePort: 8000\n        path: &#x2F;\n        pathType: ImplementationSpecific</code></pre>\n<p>将域名<code>console.kubegems.io</code>解析到 Kubernetes 集群内任意节点后，即可通过 <code>http://console.kubegems.io:&lt;NodePort&gt;</code> 访问页面。推荐使用<a href=\"https://swh.app/\">switchhost</a>设置域名解析。</p>\n<h3 id=\"系统命名空间\"><a href=\"#系统命名空间\" class=\"headerlink\" title=\"系统命名空间\"></a>系统命名空间</h3><table>\n<thead>\n<tr>\n<th>命名空间</th>\n<th>用途</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>cert-manager</td>\n<td>证书管理所在空间</td>\n</tr>\n<tr>\n<td>kubegems-installer</td>\n<td>KubeGems installer 所在空间</td>\n</tr>\n<tr>\n<td>gemcloud-system</td>\n<td>KubeGems系统服务所在空间</td>\n</tr>\n<tr>\n<td>gemcloud-monitoring-system</td>\n<td>监控告警服务所在空间</td>\n</tr>\n<tr>\n<td>gemcloud-logging-system</td>\n<td>日志服务所在空间</td>\n</tr>\n<tr>\n<td>gemcloud-gateway-system</td>\n<td>租户网关所在空间</td>\n</tr>\n<tr>\n<td>gemcloud-workflow-system</td>\n<td>ci&#x2F;cd引擎所在空间</td>\n</tr>\n<tr>\n<td>observability</td>\n<td>可观察组件所在空间</td>\n</tr>\n<tr>\n<td>istio-system</td>\n<td>istio 组件所在空间</td>\n</tr>\n<tr>\n<td>local-path-storage</td>\n<td>host pv服务所在空间</td>\n</tr>\n</tbody></table>\n<h2 id=\"Development\"><a href=\"#Development\" class=\"headerlink\" title=\"Development\"></a>Development</h2><h3 id=\"Run-local\"><a href=\"#Run-local\" class=\"headerlink\" title=\"Run local\"></a>Run local</h3><p>kubegems have 5 components:</p>\n<ul>\n<li>service: provide kubegems api server.</li>\n<li>msgbus: provide instant communication for <code>service</code>, <code>agent</code> and <code>dashboard</code>.</li>\n<li>worker: execute long time task.</li>\n<li>agent: proxy all request by service in a single cluster.</li>\n<li>controller: reconcile all kubegems CRD requests.</li>\n</ul>\n<p>Choose one of these component you want to run, then:</p>\n<ol>\n<li>prepare certs: <code>cd scripts &amp;&amp; bash generate-tls-certs.sh</code></li>\n<li><code>make build</code></li>\n<li><code>./bin/kubegems &#123;component&#125; gencfg &gt; config/config.yaml</code></li>\n<li>Modify <code>config/config.yaml</code> yourself, for different component, config.yaml is different, you can also use args or enironment variables.</li>\n<li><code>./bin/kubegems &#123;conpoment&#125;</code></li>\n</ol>\n<h2 id=\"更多配置\"><a href=\"#更多配置\" class=\"headerlink\" title=\"更多配置\"></a>更多配置</h2><h3 id=\"Linux内核优化\"><a href=\"#Linux内核优化\" class=\"headerlink\" title=\"Linux内核优化\"></a>Linux内核优化</h3><p>inux kernel参数优化是为KubeGems worker节点更好、稳定的服务容器而总结出来的一套内核优化基线。其中部分参数涉及到机器性能的动态调整，文档里会给出计算公式，请读者自行根据情况转换。</p>\n<pre class=\"line-numbers language-none\"><code class=\"language-none\"># 计算内存总量mem_total_bytes &#x3D; memtotal_mb * 1024 * 1024</code></pre>\n\n<h4 id=\"修改sysctl-conf\"><a href=\"#修改sysctl-conf\" class=\"headerlink\" title=\"修改sysctl.conf\"></a>修改<a href=\"\">sysctl.conf</a></h4><p>使用vim打开并编辑<code>/etc/sysctl.conf</code>文件</p>\n<pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">kernel.panic &#x3D; 10kernel.panic_on_oops &#x3D; 10kernel.sysrq&#x3D;0\n# 最大进程数#用命令ps -eLf | wc -l 查看当前实际 PID 数量，检查当前用户的 PID 数量\nkernel.pid_max&#x3D;196605\n#最大线程数\nkernel.threads_max&#x3D;196605\nkernel.ctrl-alt-del&#x3D;1\n#打开coredns\nkernel.core_pattern&#x3D;&#x2F;var&#x2F;log&#x2F;core.%e.%p.%t\nkernel.shmmax &#x3D; &#123;&#123; mem_total_bytes&#x2F;&#x2F;2 &#125;&#125;\nkernel.shmall &#x3D; &#123;&#123; mem_total_bytes&#x2F;&#x2F;2&#x2F;&#x2F;4096 &#125;&#125;\nkernel.msgmnb &#x3D; 65536\nkernel.msgmax &#x3D; 65536\nfs.file-max &#x3D; 1048576\n#每个 linux 进程可以持有多个 fd，每个 inotify 类型的 fd 可以 watch 多个目录，每个用户下所有进程 inotify 类型的 fd 可以 watch 的总目录数有个最大限制\nfs.inotify.max_user_instances&#x3D;524288\nfs.inotify.max_user_watches&#x3D;524288\n#如果wattch数过多可以打开 inotify_add_watch 跟踪，进一步 debug inotify watch 耗尽的原因:\n#echo 1 &gt;&gt; &#x2F;sys&#x2F;kernel&#x2F;debug&#x2F;tracing&#x2F;events&#x2F;syscalls&#x2F;sys_exit_inotify_add_watch&#x2F;enable\n#关闭swap空间\nvm.swappiness &#x3D; 0\nvm.max_map_count&#x3D;1048575\nvm.dirty_ratio &#x3D; 60\nvm.dirty_background_ratio &#x3D; 5\nvm.min_free_kbytes &#x3D; &#123;&#123; mem_total_bytes&#x2F;&#x2F;1024&#x2F;&#x2F;100*5 &#125;&#125;\nnet.ipv4.tcp_syncookies &#x3D; 1\nnet.ipv4.tcp_synack_retries &#x3D; 2\nnet.ipv4.tcp_dsack &#x3D; 1\nnet.ipv4.tcp_sack &#x3D; 0\nnet.ipv4.tcp_fack &#x3D; 1\nnet.ipv4.conf.all.forwarding &#x3D; 1\nnet.ipv4.conf.default.forwarding &#x3D; 1\nnet.ipv4.ip_forward &#x3D; 1\nnet.ipv4.conf.all.send_redirects &#x3D; 0\nnet.ipv4.conf.default.send_redirects &#x3D; 0\nnet.ipv4.conf.all.accept_source_route &#x3D; 0\nnet.ipv4.conf.default.accept_source_route &#x3D; 0\nnet.ipv4.conf.all.rp_filter &#x3D; 0\nnet.ipv4.conf.default.rp_filter &#x3D; 0\nnet.ipv4.conf.all.accept_redirects &#x3D; 0\nnet.ipv4.conf.default.accept_redirects &#x3D; 0\nnet.ipv4.conf.all.secure_redirects &#x3D; 0\nnet.ipv4.conf.default.secure_redirects &#x3D; 0\nnet.ipv4.conf.all.bootp_relay &#x3D; 0net.ipv4.conf.all.proxy_arp &#x3D; 0\nnet.ipv4.icmp_echo_ignore_broadcasts &#x3D; 1\nnet.ipv4.icmp_ignore_bogus_error_responses &#x3D; 1\nnet.ipv4.tcp_rfc1337 &#x3D; 1\nnet.ipv4.tcp_congestion_control &#x3D; cubic\nnet.core.default_qdisc &#x3D; pfifo_fast\nnet.ipv4.tcp_ecn &#x3D; 2\nnet.ipv4.tcp_reordering &#x3D; 3\nnet.ipv4.tcp_retries2 &#x3D; 8\nnet.ipv4.tcp_retries1 &#x3D; 3\nnet.ipv4.tcp_no_metrics_save &#x3D; 1\nnet.ipv4.tcp_slow_start_after_idle &#x3D; 0\nnet.ipv4.tcp_fin_timeout &#x3D; 10\n#tcp_keepalive_time需要低于ipvs中的tcp_timeout时长，一般情况下要低于kube-proxy lvs的900s\nnet.ipv4.tcp_keepalive_time &#x3D; 600\nnet.ipv4.tcp_keepalive_probes &#x3D; 5\nnet.ipv4.tcp_keepalive_intvl &#x3D; 15\nnet.ipv4.ip_local_port_range &#x3D; 20000 65535\n#预留给kubernetes service的nodeport端口范围，不设置可能会造成#kubernetes在做服务探针时使用下列范围端口，造成连接被占用而失败，引起探针失效\nnet.ipv4.ip_local_reserved_ports &#x3D; 30000-32768\n# 每个网络接口接收数据包的速率比内核处理这些包的速率快时，允许送到队列的数据包的最大数目\nnet.core.netdev_max_backlog &#x3D; 16384\n#没有启用syncookies的情况下，syn queue(半连接队列)大小除了受somaxconn限制外，也受这个参数的限制，默认1024，优化到65535，避免在高并发场景下丢包\nnet.ipv4.tcp_max_syn_backlog &#x3D; 65535\n# 表示socket监听(listen)的backlog上限，也就是就是socket的监听队列(accept queue)，当一个tcp连接尚未被处理或建立时(半连接状态)\n#会保存在这个监听队列，默认为 128，在高并发场景下偏小，优化到 16384。参考 https:&#x2F;&#x2F;imroc.io&#x2F;posts&#x2F;kubernetes-overflow-and-drop&#x2F;\nnet.core.somaxconn &#x3D; 16384\nnet.ipv4.tcp_window_scaling &#x3D; 1\nnet.ipv4.tcp_adv_win_scale &#x3D; 2\nnet.ipv4.tcp_rmem &#x3D; 4096 524287 16777216\nnet.core.rmem_default &#x3D; 524287\nnet.core.rmem_max &#x3D; 16777216\nnet.ipv4.tcp_wmem &#x3D; 4096 524287 16777216\nnet.core.wmem_default &#x3D; 524287\nnet.core.wmem_max &#x3D; 16777216\nnet.core.optmem_max &#x3D; 524287\nnet.ipv4.tcp_fastopen &#x3D; 3\nnet.ipv4.tcp_timestamps &#x3D; 1\nnet.ipv4.tcp_tw_recycle &#x3D; 0\nnet.ipv4.tcp_tw_reuse &#x3D; 0\nnet.ipv4.tcp_max_tw_buckets &#x3D; 360000\n# 当前 arp 记录数接近 gc_thresh3 比较容易 overflow，因为当 arp 记录达到 gc_thresh3 时会强制触发 gc 清理#当这时又有数据包要发送，并且根据目的 IP 在 arp cache 中没找到 mac 地址，这时会判断当前 arp cache 记录数加 1 是否大于 gc_thresh3#如果没有大于就会 时就会报错: arp_cache: neighbor table overflow!# arp -an | wc -l参看arp记录数\nnet.ipv4.neigh.default.gc_thresh3 &#x3D; 2048\nnet.ipv4.neigh.default.gc_thresh2 &#x3D; 1024\nnet.ipv4.neigh.default.gc_thresh1 &#x3D; 128\n#分析如果是业务服务常见性的出现&quot;arp_cache: neighbor table overflow!&quot;，则考虑推送将下列注释的参数推倒所有Node节点\n#net.ipv4.neigh.default.gc_thresh1 &#x3D; 80000\n#net.ipv4.neigh.default.gc_thresh2 &#x3D; 90000\n#net.ipv4.neigh.default.gc_thresh3 &#x3D; 100000\nnet.ipv4.neigh.default.gc_interval &#x3D; 120\nnet.ipv4.route.flush &#x3D; 1\nnet.ipv4.rt_cache_rebuild_count &#x3D; -1\nnet.netfilter.nf_conntrack_max &#x3D; 4194304net.nf_conntrack_max &#x3D; 4194304\nnet.netfilter.nf_conntrack_buckets &#x3D; 1048576\nnet.netfilter.nf_conntrack_tcp_timeout_fin_wait&#x3D;30\nnet.netfilter.nf_conntrack_tcp_timeout_time_wait&#x3D;300\nnet.netfilter.nf_conntrack_tcp_timeout_close_wait&#x3D;1\n#维持通过NAT维持TCP长连接的优化,注意kube-proxy会修改此参数\nnet.netfilter.nf_conntrack_tcp_timeout_established&#x3D;3600\n#tcp_keepalive_time+ tcp_keepalive_interval * tcp_keepalive_max_retry + 2msl取整\n#https:&#x2F;&#x2F;www.xinmeow.com&#x2F;2020&#x2F;05&#x2F;04&#x2F;iptables-nf_conntrack-%E6%9D%A1%E7%9B%AE%E7%9A%84%E8%80%81%E5%8C%96%E6%97%B6%E9%97%B4%E5%9B%A0%E8%AF%A5%E8%BF%9E%E6%8E%A5%E5%8F%91%E7%94%9F%E4%B8%A2%E5%8C%85%E8%80%8C%E5%8F%98%E7%9F%AD%EF%BC%8C&#x2F;\nnet.netfilter.nf_conntrack_tcp_timeout_max_retrans&#x3D;720\nnet.ipv6.conf.all.disable_ipv6 &#x3D; 1\nnet.ipv6.conf.default.disable_ipv6 &#x3D; 1</code></pre>\n\n<h4 id=\"生效配置\"><a href=\"#生效配置\" class=\"headerlink\" title=\"生效配置\"></a>生效配置</h4><p>执行<code>sysctl -p</code>命令即时生效配置<br>或者<br>执行<code>reboot</code>重启服务器后内核配置生效</p>\n<h3 id=\"debug-by-vscode\"><a href=\"#debug-by-vscode\" class=\"headerlink\" title=\"debug by vscode\"></a>debug by vscode</h3><pre class=\"line-numbers language-bash\" data-language=\"bash\"><code class=\"language-bash\">&#123;\n  &quot;name&quot;: &quot;service&quot;,\n  &quot;type&quot;: &quot;go&quot;,\n  &quot;request&quot;: &quot;launch&quot;,\n  &quot;mode&quot;: &quot;debug&quot;,\n  &quot;program&quot;: &quot;$&#123;workspaceFolder&#125;&#x2F;cmd&quot;,\n  &quot;cwd&quot;: &quot;$&#123;workspaceFolder&#125;&quot;, \n  &quot;args&quot;: [&quot;service&quot;] &#x2F;&#x2F; may also be msgbus, worker, agent, controller\n&#125;</code></pre>","text":"概述KubeGems是一款以围绕 Kubernetes 通过自研和集成云原生项目而构建的通用性开源 PaaS 云管理平台。经过我们内部近一年的持续迭代，当前 KubeGems 的核心功能已经初步具备多云多租户场景下的统一管理。并通过插件化的方式，在用户界面中灵活控制包括 监控系统...","link":"","photos":[],"count_time":{"symbolsCount":"8.6k","symbolsTime":"8 mins."},"categories":[{"name":"Cloud-Native","slug":"Cloud-Native","count":6,"path":"api/categories/Cloud-Native.json"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","count":3,"path":"api/tags/Kubernetes.json"},{"name":"k8s-dashboard","slug":"k8s-dashboard","count":3,"path":"api/tags/k8s-dashboard.json"},{"name":"Docker","slug":"Docker","count":2,"path":"api/tags/Docker.json"},{"name":"kubegem","slug":"kubegem","count":1,"path":"api/tags/kubegem.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%A6%82%E8%BF%B0\"><span class=\"toc-text\">概述</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E5%8A%9F%E8%83%BD%E9%A2%84%E8%A7%88\"><span class=\"toc-text\">功能预览</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#quick-start\"><span class=\"toc-text\">quick start</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#installer\"><span class=\"toc-text\">installer</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E8%AE%BF%E9%97%AE%E9%A1%B5%E9%9D%A2\"><span class=\"toc-text\">访问页面</span></a></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#%E7%B3%BB%E7%BB%9F%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4\"><span class=\"toc-text\">系统命名空间</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Development\"><span class=\"toc-text\">Development</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Run-local\"><span class=\"toc-text\">Run local</span></a></li></ol></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#%E6%9B%B4%E5%A4%9A%E9%85%8D%E7%BD%AE\"><span class=\"toc-text\">更多配置</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#Linux%E5%86%85%E6%A0%B8%E4%BC%98%E5%8C%96\"><span class=\"toc-text\">Linux内核优化</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E4%BF%AE%E6%94%B9sysctl-conf\"><span class=\"toc-text\">修改sysctl.conf</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E7%94%9F%E6%95%88%E9%85%8D%E7%BD%AE\"><span class=\"toc-text\">生效配置</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#debug-by-vscode\"><span class=\"toc-text\">debug by vscode</span></a></li></ol></li></ol>","author":{"name":"Moses","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"一位正在重塑知识的技术人","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"熔断、超时、限流和服务降级的概念梳理","uid":"413e999854a8918e45b13a69d229a398","slug":"熔断、超时、限流和服务降级","date":"2022-05-05T07:42:08.000Z","updated":"2022-10-27T03:00:16.565Z","comments":true,"path":"api/articles/熔断、超时、限流和服务降级.json","keywords":null,"cover":"https://gtwallpaper.org/sites/default/files/wallpaper/159416/4k-beneath-the-clouds-wallpapers-159416-910142-2825240.png","text":"服务熔断解决的问题 解决微服务架构下，系统间调用链路的雪崩，避免造成整个微服务体系崩溃。类似电路过载的保险丝。 技术、架构类问题的解决方案，多数研发和运维参与。 从局部出发，各自解决问题，被动防御。 造成链路调用雪崩的原因 调用链的某一环节，例如APP-D出现性能瓶颈，导致依赖该...","link":"","photos":[],"count_time":{"symbolsCount":"1.4k","symbolsTime":"1 mins."},"categories":[{"name":"分布式","slug":"分布式","count":6,"path":"api/categories/分布式.json"},{"name":"微服务","slug":"分布式/微服务","count":3,"path":"api/categories/分布式/微服务.json"}],"tags":[{"name":"服务限流","slug":"服务限流","count":2,"path":"api/tags/服务限流.json"},{"name":"服务熔断","slug":"服务熔断","count":1,"path":"api/tags/服务熔断.json"},{"name":"服务降级","slug":"服务降级","count":1,"path":"api/tags/服务降级.json"}],"author":{"name":"Moses","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"一位正在重塑知识的技术人","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"容器安全平台-deepfence-ThreatMapper","uid":"f8440a3d154328a49baadf9056bbbcfd","slug":"容器安全平台-deepfence-ThreatMapper","date":"2022-04-21T03:36:00.000Z","updated":"2022-10-27T02:15:45.038Z","comments":true,"path":"api/articles/容器安全平台-deepfence-ThreatMapper.json","keywords":null,"cover":"https://gtwallpaper.org/sites/default/files/wallpaper/159422/4k-empty-road-new-wallpapers-159422-9856-8946046.png","text":"容器安全平台-deepfence0x01简介ThreatMapper 开源云原生安全可观测性平台 0x02架构图Deepfence Threat由两部分组成： Management Console:管理端包括应用拓扑、一般SBOMs漏洞、威胁地图 ThreatMapper Sen...","link":"","photos":[],"count_time":{"symbolsCount":"2.8k","symbolsTime":"3 mins."},"categories":[{"name":"Cloud-Native-Security","slug":"Cloud-Native-Security","count":12,"path":"api/categories/Cloud-Native-Security.json"}],"tags":[{"name":"容器安全平台","slug":"容器安全平台","count":5,"path":"api/tags/容器安全平台.json"},{"name":"产品选型","slug":"产品选型","count":6,"path":"api/tags/产品选型.json"},{"name":"开源云原生安全","slug":"开源云原生安全","count":1,"path":"api/tags/开源云原生安全.json"},{"name":"DeepFence","slug":"DeepFence","count":1,"path":"api/tags/DeepFence.json"},{"name":"云原生威胁地图","slug":"云原生威胁地图","count":1,"path":"api/tags/云原生威胁地图.json"}],"author":{"name":"Moses","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"一位正在重塑知识的技术人","socials":{"github":"","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}